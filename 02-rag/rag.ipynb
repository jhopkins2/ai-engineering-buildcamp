{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d26cc8b",
   "metadata": {},
   "source": [
    "# Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bc9083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m120 packages\u001b[0m \u001b[2min 0.46ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m116 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add requests python-frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6946bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c8ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_owner = \"evidentlyai\"\n",
    "repo_name = \"docs\"\n",
    "branch_name = \"main\"\n",
    "\n",
    "zip_url = f\"https://github.com/{repo_owner}/{repo_name}/archive/refs/heads/{branch_name}.zip\"\n",
    "zip_response = requests.get(zip_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0357866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "with zipfile.ZipFile(io.BytesIO(zip_response.content)) as zip_archive:\n",
    "    for file_path in zip_archive.namelist():\n",
    "        if not file_path.endswith(('.md', '.mdx')):\n",
    "            continue\n",
    "        with zip_archive.open(file_path) as file:\n",
    "            content = file.read().decode('utf-8')\n",
    "            post = frontmatter.loads(content)\n",
    "            doc = {\n",
    "                'content': post.content,\n",
    "                'title':post.metadata.get('title'),\n",
    "                'description': post.metadata.get('description'),\n",
    "                'filename': file_path.split('/', 1)[-1]\n",
    "            }\n",
    "            documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75cb7672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "767af4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 95 documents\n"
     ]
    }
   ],
   "source": [
    "from gitsource import GithubRepositoryDataReader\n",
    "\n",
    "reader = GithubRepositoryDataReader(\n",
    "    repo_name=\"docs\",\n",
    "    repo_owner=\"evidentlyai\",\n",
    "    allowed_extensions=(\"md\", \"mdx\")\n",
    ")\n",
    "\n",
    "files = reader.read()\n",
    "\n",
    "print(f\"Loaded {len(files)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a24e7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [f.parse() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59258315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Output formats', 'description': 'How to export the evaluation results.', 'content': 'You can view or export Reports in multiple formats.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Log to Workspace\\n\\nYou can save the computed Report in Evidently Cloud or your local workspace.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n<Info>\\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\\n</Info>\\n\\n## View in Jupyter notebook\\n\\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\\n\\nAfter running the Report, simply call the resulting Python object:\\n\\n```python\\nmy_report\\n```\\n\\nThis will render the HTML object directly in the notebook cell.\\n\\n## HTML\\n\\nYou can also save this interactive visual Report as an HTML file to open in a browser:\\n\\n```python\\nmy_report.save_html(“file.html”)\\n```\\n\\nThis option is useful for sharing Reports with others or if you\\'re working in a Python environment that doesn’t display interactive visuals.\\n\\n## JSON\\n\\nYou can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\\n\\nTo view the JSON in Python:\\n\\n```python\\nmy_report.json()\\n```\\n\\nTo save the JSON as a separate file:\\n\\n```python\\nmy_report.save_json(\"file.json\")\\n```\\n\\n## Python dictionary\\n\\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\\n\\nTo get the dictionary:\\n\\n```python\\nmy_report.dict()\\n```', 'filename': 'docs/library/output_formats.mdx'}\n"
     ]
    }
   ],
   "source": [
    "print(documents[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eec16c",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8301b279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m120 packages\u001b[0m \u001b[2min 0.54ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m116 packages\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5ec04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13011d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7842f82334d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=[\"title\", \"description\", \"content\"],\n",
    "    keyword_fields=[\"filename\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe294e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "query = \"LLM as a judge\"\n",
    "\n",
    "results = index.search(query=query)\n",
    "\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599ead35",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "141744aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = list(range(0, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "162c289b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "[25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "[30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "[35, 36, 37, 38, 39, 40, 41, 42, 43, 44]\n",
      "[40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "[45, 46, 47, 48, 49, 50, 51, 52, 53, 54]\n",
      "[50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
      "[55, 56, 57, 58, 59, 60, 61, 62, 63, 64]\n",
      "[60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "[65, 66, 67, 68, 69, 70, 71, 72, 73, 74]\n",
      "[70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "[75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n",
      "[80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
      "[85, 86, 87, 88, 89, 90, 91, 92, 93, 94]\n",
      "[90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "[95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "window_size = 10\n",
    "start = 0\n",
    "step = 5\n",
    "\n",
    "while start < len(document):\n",
    "    end = start + window_size\n",
    "    chunk = document[start:end]\n",
    "    print(chunk)\n",
    "\n",
    "    start += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "638ade20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(text, size=1000, step=500):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append({'start':start, 'content':chunk})\n",
    "\n",
    "        start = end - step\n",
    "\n",
    "        if end >= text_length:\n",
    "            break\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ba67944",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    if not doc.get('content'):\n",
    "        continue\n",
    "    copy = doc.copy()\n",
    "    content = copy.pop('content')\n",
    "\n",
    "    chunks = sliding_window(content, size=3000, step=1500)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.update(copy)\n",
    "        chunk['chunk_id'] = i\n",
    "        document_chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c0a43d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 9000, 'content': 'cation=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\")],\\n    categorical_columns=[\"target\", \"prediction\"])\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: Optional[str] = None\\n    prediction_probas: Optional[str] = \"prediction\" #if probabilistic classification\\n    pos_label: Label = 1 #name of the positive label\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n### Ranking\\n\\n#### RecSys\\n\\nTo evaluate recommender systems performance, you must map the columns with:\\n\\n- Prediction: this could be predicted score or rank.\\n- Target: relevance labels (e.g., this could be an interaction result like user click or upvote, or a true relevance label)\\n\\nThe **target** column can contain either:\\n\\n- a binary label (where `1` is a positive outcome)\\n- any scores (positive values, where a higher value corresponds to a better match or a more valuable user action).\\n\\nHere are the examples of the expected data inputs.\\n\\nIf the system prediction is a **score** (expected by default):\\n\\n| user_id | item_id | prediction (score) | target (relevance) |\\n| ------- | ------- | ------------------ | ------------------ |\\n| user_1  | item_1  | 1.95               | 0                  |\\n| user_1  | item_2  | 0.8                | 1                  |\\n| user_1  | item_3  | 0.05               | 0                  |\\n\\nIf the model prediction is a **rank**:\\n\\n| user_id | item_id | prediction (rank) | target (relevance) |\\n| ------- | ------- | ----------------- | ------------------ |\\n| user_1  | item_1  | 1                 | 0                  |\\n| user_1  | item_2  | 2                 | 1                  |\\n| user_1  | item_3  | 3                 | 0                  |\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n    )\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    user_id: str = \"user_id\" #columns with user IDs\\n    item_id: str = \"item_id\" #columns with ranked items\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```', 'title': 'Data definition', 'description': 'How to map the input data.', 'filename': 'docs/library/data_definition.mdx', 'chunk_id': 6}\n"
     ]
    }
   ],
   "source": [
    "print(document_chunks[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dcd6b56f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x7842d1bc8f50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_index = Index(\n",
    "    text_fields=[\"title\", \"description\", \"content\"],\n",
    "    keyword_fields=[\"filename\"]\n",
    ")\n",
    "\n",
    "chunk_index.fit(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b89d145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "result = chunk_index.search(\"LLM as a judge\")\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62643020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 0},\n",
       " {'start': 1500,\n",
       "  'content': 'valuator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 1},\n",
       " {'start': 19500,\n",
       "  'content': 'he message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 13},\n",
       " {'start': 15000,\n",
       "  'content': ' REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFra',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 10},\n",
       " {'start': 16500,\n",
       "  'content': 'ptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s how to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey t',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 11},\n",
       " {'start': 18000,\n",
       "  'content': 'me(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s how to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically tr',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 12},\n",
       " {'start': 3000,\n",
       "  'content': 's/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quickly and easily.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I add another user to my account?\",\\n     \"I am afraid it is not currently possible to add multiple users to the account. Our system supports only one user per account for security reasons. We recommend creating separate accounts for different users.\",\\n     \"To add a secondary user, go to \\'Account Settings\\', select \\'Manage Users\\', and enter the details of the person you want to add. You can set permissions for their access, deciding what they can and cannot do within the account.\",\\n     \"incorrect\", \"contradiction (incorrect answer)\"],\\n  \\n    [\"Is it possible to link multiple bank accounts?\",\\n     \"Yes, you can link multiple bank accounts by going to \\'Account Settings\\' in the menu and selecting \\'Add Bank Account\\'. Follow the prompts to add your bank account details. Make sure to verify each bank account by following the verification process.\",\\n     \"You can add multiple bank accounts by visiting \\'Accounts\\' in the menu and choosing \\'',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 2},\n",
       " {'start': 13500,\n",
       "  'content': '_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descri',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 9},\n",
       " {'start': 12000,\n",
       "  'content': 'entication, log into your account, go to \\'Security Settings\\', and select \\'Enable 2FA\\'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\\n     \"To enable two-factor authentication, log into your account, navigate to \\'Security Settings\\', and choose \\'Enable 2FA\\'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\\n     \"incorrect\", \"adds new information (backup codes)\"]\\n  ]\\n  \\n  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\\n  \\n  golden_dataset = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n<Note>\\n  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\n**Create an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 8},\n",
       " {'start': 10500,\n",
       "  'content': ' changes, and you will receive a confirmation email with the updated address details.\",\\n     \"incorrect\", \"adds new information (confirmation email)\"],\\n  \\n    [\"How do I contact customer support?\",\\n     \"You can contact customer support by logging into your account, going to the \\'Help\\' section, and selecting \\'Contact Us\\'. You can choose to reach us via email, phone, or live chat for immediate assistance.\",\\n     \"To contact customer support, log into your account and go to the \\'Help\\' section. Select \\'Contact Us\\' and choose your preferred method: email, phone, or live chat. Our support team is available 24/7 to assist you with any issues. Additionally, you can find a FAQ section that may answer your questions without needing to contact support.\",\\n     \"incorrect\", \"adds new information (24/7 availability, FAQ section)\"],\\n  \\n    [\"What should I do if my card is lost or stolen?\",\\n     \"If your card is lost or stolen, immediately log into your account, go to \\'Card Management\\', and select \\'Report Lost/Stolen\\'. Follow the instructions to block your card and request a replacement. You can also contact our support team for assistance.\",\\n     \"If your card is lost or stolen, navigate to \\'Card Management\\' in your account, and select \\'Report Lost/Stolen\\'. Follow the prompts to block your card and request a replacement. Additionally, you can contact our support team for help.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I enable two-factor authentication (2FA)?\",\\n     \"To enable two-factor authentication, log into your account, go to \\'Security Settings\\', and select \\'Enable 2FA\\'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\\n     \"To enable two-factor authentication, log into your account, navigate to \\'Security Settings\\', and choose \\'Enable 2FA\\'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\\n     \"incorrect\", \"adds new information (backup codes)\"]\\n  ]\\n  \\n  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\\n  \\n  golden_dataset = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n<Note>\\n  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\n**Create an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 7}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e047ec0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 4000,\n",
       " 'content': 'apping**                               |\\n| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- |\\n| `numerical_columns`   | <ul>      <li>      Columns with numeric values.</li>            </ul>                                                                | All columns with numeric types (`np.number`).       |\\n| `datetime_columns`    | <ul>      <li>      Columns with datetime values.</li>            <li>      Ignored in data drift calculations.</li>            </ul> | All columns with DateTime format (`np.datetime64`). |\\n| `categorical_columns` | <ul>      <li>      Columns with categorical values.</li>            </ul>                                                            | All non-numeric/non-datetime columns.               |\\n| `text_columns`        | <ul>      <li>      Text columns.</li>            <li>      Mapping required for text data drift detection.</li>            </ul>     | No automated mapping.                               |\\n\\n### ID and timestamp\\n\\nIf you have a timestamp or ID column, it\\'s useful to identify them.\\n\\n```python\\ndefinition = DataDefinition(\\n    id_column=\"Id\",\\n    timestamp=\"Date\"\\n    )\\n```\\n\\n| **Column role** | **Description**                                                                                                             | **Automated mapping**    |\\n| --------------- | --------------------------------------------------------------------------------------------------------------------------- | ------------------------ |\\n| `id_column`     | <ul>      <li>      Identifier column.</li>            <li>      Ignored in data drift calculations.</li>            </ul>  | Column named \"id\"        |\\n| `timestamp`     | <ul>      <li>      Timestamp column.</li>            <li>       Ignored in data drift calculations. </li>            </ul> | Column named \"timestamp\" |\\n\\n<In',\n",
       " 'title': 'Data definition',\n",
       " 'description': 'How to map the input data.',\n",
       " 'filename': 'docs/library/data_definition.mdx'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gitsource import chunk_documents\n",
    "document_chunks = chunk_documents(documents)\n",
    "document_chunks[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a84170",
   "metadata": {},
   "source": [
    "# Augmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0c95d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do I implement an LLM as a judge?\"\n",
    "search_result = chunk_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0e8d50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "search_result_json = json.dumps(search_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dc0424be",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're a course assistant, your task is to anser the QUESTION from\n",
    "course students using the provided CONTEXT\n",
    "\"\"\".strip()\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "<QUESTION>\n",
    "{query}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{search_result_json}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b695f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(client, user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "    \n",
    "    if instructions is not None:\n",
    "        messages.append({\n",
    "            \"role\":\"system\", \n",
    "            \"content\":instructions\n",
    "        })\n",
    "    \n",
    "    messages.append({\n",
    "        \"role\":\"user\", \n",
    "        \"content\":user_prompt\n",
    "    })\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47b5f0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement an LLM (Large Language Model) as a judge, you can follow these steps based on the tutorial provided:\n",
      "\n",
      "1. **Install Required Libraries**:\n",
      "   - Use the following command to install the Evidently library:\n",
      "     ```bash\n",
      "     pip install evidently\n",
      "     ```\n",
      "\n",
      "2. **Import Necessary Modules**:\n",
      "   - In your Python environment, import the necessary libraries:\n",
      "     ```python\n",
      "     import pandas as pd\n",
      "     import numpy as np\n",
      "     from evidently import Dataset, DataDefinition, Report, BinaryClassification\n",
      "     from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "     ```\n",
      "\n",
      "3. **Set Up OpenAI API Key**: \n",
      "   - Assign your OpenAI API key as an environment variable:\n",
      "     ```python\n",
      "     import os\n",
      "     os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "     ```\n",
      "\n",
      "4. **Create an Evaluation Dataset**:\n",
      "   - Generate a toy Q&A dataset with questions, target responses, new responses, and manual labels indicating correctness.\n",
      "\n",
      "5. **Define the LLM Judge Prompt**:\n",
      "   - Design a prompt that the LLM will use to evaluate the responses. For example:\n",
      "     ```python\n",
      "     correctness = BinaryClassificationPromptTemplate(\n",
      "         criteria=\"Your criteria for correctness.\",\n",
      "         target_category=\"correct\",\n",
      "         non_target_category=\"incorrect\",\n",
      "         pre_messages=[(\"system\", \"You are an expert evaluator.\")]\n",
      "     )\n",
      "     ```\n",
      "\n",
      "6. **Add Descriptors to Dataset**:\n",
      "   - Include the LLM evaluator to your dataset:\n",
      "     ```python\n",
      "     eval_dataset.add_descriptors(descriptors=[\n",
      "         LLMEval(\"new_response\",\n",
      "                 template=correctness,\n",
      "                 provider=\"openai\",\n",
      "                 model=\"gpt-4o-mini\",\n",
      "                 alias=\"Correctness\",\n",
      "                 additional_columns={\"target_response\": \"target_response\"}),\n",
      "     ])\n",
      "     ```\n",
      "\n",
      "7. **Run Evaluation and Generate Reports**:\n",
      "   - Create a report to summarize the results of the evaluation:\n",
      "     ```python\n",
      "     report = Report([\n",
      "         TextEvals()\n",
      "     ])\n",
      "     my_eval = report.run(eval_dataset, None)\n",
      "     ```\n",
      "\n",
      "8. **Iterate and Improve**:\n",
      "   - Review the results and adjust the prompt or model as needed to optimize evaluations.\n",
      "\n",
      "9. **Next Steps**: \n",
      "   - Consider integrating the LLM judge into your broader evaluation workflow, using it to assess outputs from other LLMs or configurations.\n",
      "\n",
      "This framework allows you to build a system where an LLM acts as an evaluator, applying customized criteria to determine the quality of responses. You can refer to the full [documentation on LLM judges](your_link_here) for more details and examples.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai_client = OpenAI()\n",
    "response = llm(openai_client, user_prompt, instructions)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdb4ff5",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e01062d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return chunk_index.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9a582ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(query, search_result):\n",
    "    json_search_results = json.dumps(search_result)\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    <QUESTION>\n",
    "    {query}\n",
    "    </QUESTION>\n",
    "\n",
    "    <CONTEXT>\n",
    "    {search_result_json}\n",
    "    </CONTEXT>\n",
    "    \"\"\".strip()\n",
    "\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "867a74f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(client, query, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    search_result = search(query)\n",
    "    prompt = augment(query, search_result)\n",
    "    answer = llm(client, prompt, instructions, model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9edce35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement an LLM as a judge using EvidentlyAI, follow these steps:\n",
      "\n",
      "1. **Installation and Setup**:\n",
      "   - Install the Evidently library by running:\n",
      "     ```bash\n",
      "     pip install evidently\n",
      "     ```\n",
      "   - Import the necessary modules in your Python environment:\n",
      "     ```python\n",
      "     import pandas as pd\n",
      "     import numpy as np\n",
      "     from evidently import Dataset\n",
      "     from evidently import DataDefinition\n",
      "     from evidently import Report\n",
      "     from evidently import BinaryClassification\n",
      "     from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "     ```\n",
      "\n",
      "2. **Set Up API Key**:\n",
      "   - Provide your OpenAI API key as an environment variable:\n",
      "     ```python\n",
      "     import os\n",
      "     os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "     ```\n",
      "\n",
      "3. **Create Dataset**:\n",
      "   - Create a Q&A dataset including:\n",
      "     - Questions\n",
      "     - Target responses (approved)\n",
      "     - New responses (system-generated)\n",
      "     - Manual labels indicating correctness.\n",
      "   - Use code like this to generate the dataset:\n",
      "     ```python\n",
      "     data = [\n",
      "         [\"Question\", \"Target response\", \"New response\", \"label\", \"explanation\"],\n",
      "         # Add more rows as needed\n",
      "     ]\n",
      "     ```\n",
      "\n",
      "4. **Add Descriptors for LLM Judgment**:\n",
      "   - Add descriptors for evaluating the new responses. For correctness:\n",
      "     ```python\n",
      "     eval_dataset.add_descriptors(descriptors=[\n",
      "         LLMEval(\"new_response\",\n",
      "                 template=correctness,\n",
      "                 provider=\"openai\",\n",
      "                 model=\"gpt-4o-mini\",\n",
      "                 alias=\"Correctness\",\n",
      "                 additional_columns={\"target_response\": \"target_response\"}),\n",
      "     ])\n",
      "     ```\n",
      "\n",
      "5. **Run Evaluations**:\n",
      "   - After adding descriptors, use the following to generate a report:\n",
      "     ```python\n",
      "     report = Report([TextEvals()])\n",
      "     my_eval = report.run(eval_dataset, None)\n",
      "     ```\n",
      "\n",
      "6. **Evaluate the LLM Quality**:\n",
      "   - Evaluate the quality of your LLM judge itself by comparing its responses against the manual labels:\n",
      "     ```python\n",
      "     df = eval_dataset.as_dataframe()\n",
      "     definition_2 = DataDefinition(\n",
      "         classification=[BinaryClassification(\n",
      "             target=\"label\",\n",
      "             prediction_labels=\"Correctness\",\n",
      "             pos_label=\"incorrect\")],\n",
      "         categorical_columns=[\"label\", \"Correctness\"]\n",
      "     )\n",
      "     class_dataset = Dataset.from_pandas(df, data_definition=definition_2)\n",
      "     ```\n",
      "\n",
      "7. **Create a Verbosity Evaluator (Optional)**:\n",
      "   - If you want to evaluate for conciseness:\n",
      "     ```python\n",
      "     verbosity = BinaryClassificationPromptTemplate(\n",
      "         criteria=\"Conciseness refers to the quality of being brief...\",\n",
      "         target_category=\"concise\",\n",
      "         non_target_category=\"verbose\",\n",
      "         include_reasoning=True,\n",
      "         pre_messages=[(\"system\", \"You are an expert text evaluator.\")]\n",
      "     )\n",
      "     eval_dataset.add_descriptors(descriptors=[\n",
      "         LLMEval(\"new_response\",\n",
      "                 template=verbosity,\n",
      "                 provider=\"openai\",\n",
      "                 model=\"gpt-4o-mini\",\n",
      "                 alias=\"Verbosity\")\n",
      "     ])\n",
      "     ```\n",
      "\n",
      "8. **Upload Results to Evidently Cloud (Optional)**:\n",
      "   - To explore results in the Evidently Cloud, connect and upload your eval:\n",
      "     ```python\n",
      "     from evidently.ui.workspace import CloudWorkspace\n",
      "     ws.add_run(project.id, my_eval, include_data=True)\n",
      "     ```\n",
      "\n",
      "By following these steps, you will effectively implement an LLM as a judge using EvidentlyAI, allowing you to evaluate the outputs against custom criteria or established references.\n"
     ]
    }
   ],
   "source": [
    "question =\"How do I implement an LLM as a judge using EvidentlyAI?\"\n",
    "system_prompt = \"\"\"\n",
    "You're a course assistant, your task is to anser the QUESTION from\n",
    "course students using the provided CONTEXT\n",
    "\"\"\".strip()\n",
    "answer = rag(openai_client, question, system_prompt)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-buildcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
